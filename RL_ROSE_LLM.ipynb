{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKlpTh5M73gW",
    "outputId": "e55a5f51-7e75-484a-afa7-1fa53fe295ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ROSE' already exists and is not an empty directory.\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/radical-cybertools/ROSE\n",
    "!cd ROSE && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YlVJQkPEWUGJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r67pzpkU94RW",
    "outputId": "f14756f4-2136-42f2-c93b-211748a9047e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Import `radical` modules before `logging` to avoid the application to deadlock on `fork()`!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from rose.metrics import GREATER_THAN_THRESHOLD\n",
    "from rose.rl.reinforcement_learner import SequentialReinforcementLearner\n",
    "\n",
    "from radical.asyncflow import WorkflowEngine, RadicalExecutionBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiLviW2t_MR9"
   },
   "outputs": [],
   "source": [
    "execution_engine = await RadicalExecutionBackend({'resource': 'local.localhost'})\n",
    "asyncflow = await WorkflowEngine.create(execution_engine)\n",
    "rl = SequentialReinforcementLearner(asyncflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gac6FyebUNUN"
   },
   "outputs": [],
   "source": [
    "#from google.colab import userdata\n",
    "os.environ[\"HF_TOKEN\"] = \"REDACTED\"\n",
    "os.environ[\"HF_HOME\"] = \"/anvil/scratch/x-apark4/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CbK23cBGO9ky"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x-apark4/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 02:22:56 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ciuXrlWAOyVd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.41s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#if tokenizer.pad_token is None:\n",
    "#    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token_id = 128004\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ee841deb24694baaa455011fccc0a9c1",
      "8d4a25f5d88748f8ba515e9d6258a700",
      "5fb577ecf6714a1baff199f0cca72faa",
      "187a2660573e43a599c37c1bd2f15e11",
      "0fc5c811c1e04ebf89a9343496b251ed",
      "cca0ce8c9d814e0ab32b4d73c2acb976",
      "1a5fc6ac86b248ecb600b246013ce900",
      "6330b991bf084f93b3dc2cef1539ace6",
      "584260c925be4ba4afa16dc147344bd0",
      "1c1a8717b68146f9a22610fdc3ec8ba3",
      "044a12774231424eb85419b2085f27b4"
     ]
    },
    "id": "hDSyYVhKNAQ-",
    "outputId": "15d52250-b4fb-44b3-fcaa-b6a4ab597b20"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"qwedsacf/competition_math\", split=\"train\")\n",
    "def to_prompt_completion(example):\n",
    "    return {\n",
    "        \"prompt\": (\n",
    "            example['problem']\n",
    "        ),\n",
    "        \"ground_truth\": str(example[\"solution\"]).strip(),\n",
    "    }\n",
    "mapped = dataset.map(to_prompt_completion, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IwoDvwnjOtf7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(expr: str):\n",
    "    \"\"\"Basic normalization for math strings.\"\"\"\n",
    "    if expr is None:\n",
    "        return None\n",
    "    expr = expr.strip()\n",
    "    if expr.startswith(\"\\\\boxed{\") and expr.endswith(\"}\"):\n",
    "        expr = expr[len(\"\\\\boxed{\"):-1]\n",
    "    return expr.replace(\" \", \"\")\n",
    "def get_answer(expr: str):\n",
    "    match = re.search(r\"\\\\boxed\\{(.+?)\\}\", expr)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def compute_reward(prompts, completions, ground_truth, **kwargs):\n",
    "    pred = [normalize(c) for c in completions]\n",
    "    ref = [normalize(g) for g in ground_truth]\n",
    "    return [1.0 if p == r else 0.0 for p, r in zip(pred, ref)]\n",
    "\n",
    "def rewards_func(prompts, completions, ground_truth, **kwargs):\n",
    "    rewards = []\n",
    "    for prompt, completion, ground in zip(prompts, completions, ground_truth):\n",
    "        c = get_answer(completion)\n",
    "        g = get_answer(ground)\n",
    "        reward = 1.0 if g == c else 0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def reward_func(prompt, completions, ground_truth, **kwargs):\n",
    "    c = get_answer(completions)\n",
    "    g = get_answer(ground_truth)\n",
    "    reward = 1 if g == c else 0\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'Find the product of all $x$ such that the expression $\\\\frac{x^2+2x+1}{x^2+2x-3}$ is undefined.', 'level': 'Level 3', 'type': 'Algebra', 'solution': 'The expression is only undefined when the denominator is equal to zero. Therefore, the goal is to find the product of all real $x$ that satisfy the equation $x^2+2x-3=0$. Since the discriminant of this quadratic is $2^2 - 4(1)(-3) = 16$, which is positive, we know that the roots of $x^2 +2x-3$ are distinct real numbers.  The product of the roots of a quadratic of the form of $ax^2+bx+c$ is equal to $\\\\frac{c}{a}$, so the desired product of the values of $x$ for which $x^2 + 2x - 3=0$ is $\\\\frac{-3}{1}$, or $\\\\boxed{-3}$.'}\n",
      "To find the product of all x such that the expression is undefined, we need to find the values of x that make the denominator equal to zero.\n",
      "\n",
      "Step 1: Factor the denominator\n",
      "x^2 + 2x - 3 = (x + 3)(x - 1)\n",
      "\n",
      "Step 2: Set the denominator equal to zero and solve for x\n",
      "(x + 3)(x - 1) = 0\n",
      "\n",
      "This equation is true when either (x + 3) = 0 or (x - 1) = 0.\n",
      "\n",
      "Step 3: Solve for x\n",
      "x + 3 = 0 --> x = -3\n",
      "x - 1 = 0 --> x = 1\n",
      "\n",
      "Step 4: Calculate the product of all x\n",
      "Product = x1 * x2 = -3 * 1 = -3\n",
      "\n",
      "Therefore, the product of all x such that the expression is undefined is -3.\n",
      "\n",
      "\\boxed{-3}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = random.choice(dataset)\n",
    "print(sample)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{sample[\"problem\"]}?\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\"}\n",
    "]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=2048)\n",
    "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_func([\"Problem: Solve the equation $2x + 3 = 7$. Solution:\", \"Problem: Solve the equation $3x - 5 = 10$.\"],[\"\\\\boxed{2}\",result] ,[\"\\\\boxed{2}\",sample[\"solution\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   21/37500 01:59 < 65:17:15, 0.16 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m training_args = GRPOConfig(output_dir=\u001b[33m\"\u001b[39m\u001b[33mr1math-output\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m trainer = GRPOTrainer(\n\u001b[32m      3\u001b[39m     model=model,\n\u001b[32m      4\u001b[39m     reward_funcs=rewards_func,\n\u001b[32m      5\u001b[39m     args=training_args,\n\u001b[32m      6\u001b[39m     train_dataset=mapped,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3790\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3787\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.optimizer.train):\n\u001b[32m   3788\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.train()\n\u001b[32m-> \u001b[39m\u001b[32m3790\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   3792\u001b[39m     loss_mb = smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/trl/extras/profiling.py:98\u001b[39m, in \u001b[36mprofiling_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func.\u001b[34m__name__\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:980\u001b[39m, in \u001b[36mGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, generation_batch)\u001b[39m\n\u001b[32m    977\u001b[39m generate_every = \u001b[38;5;28mself\u001b[39m.args.steps_per_generation * \u001b[38;5;28mself\u001b[39m.num_iterations\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step % generate_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffered_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     generation_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m     generation_batch = split_pixel_values_by_grid(generation_batch)\n\u001b[32m    982\u001b[39m     generation_batch = shuffle_sequence_dict(generation_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/trl/trainer/grpo_trainer.py:1289\u001b[39m, in \u001b[36mGRPOTrainer._generate_and_score_completions\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1281\u001b[39m     profiling_context(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.generate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1282\u001b[39m     unwrap_model_for_generation(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1286\u001b[39m     FSDP.summon_full_params(\u001b[38;5;28mself\u001b[39m.model_wrapped, recurse=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_enabled \u001b[38;5;28;01melse\u001b[39;00m nullcontext(),\n\u001b[32m   1287\u001b[39m ):\n\u001b[32m   1288\u001b[39m     prompt_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m], prompt_inputs[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] = prompt_ids, prompt_mask\n\u001b[32m-> \u001b[39m\u001b[32m1289\u001b[39m     prompt_completion_ids = \u001b[43munwrapped_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_compile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   1291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[32m   1293\u001b[39m prompt_length = prompt_ids.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2617\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2616\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2617\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2628\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2629\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2630\u001b[39m         input_ids=input_ids,\n\u001b[32m   2631\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2632\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2633\u001b[39m         **model_kwargs,\n\u001b[32m   2634\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:3601\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3599\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3600\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3604\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3605\u001b[39m     outputs,\n\u001b[32m   3606\u001b[39m     model_kwargs,\n\u001b[32m   3607\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3608\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:460\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    442\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    443\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    445\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    458\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/generic.py:1083\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1081\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:390\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    402\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    403\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    404\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:304\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m residual = hidden_states\n\u001b[32m    303\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:152\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(output_dir=\"r1math-output\")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=rewards_func,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5FDXsg_2WK5g"
   },
   "outputs": [],
   "source": [
    "#manual GRPO training\n",
    "def grpo_rlvr_step(problem, true_answer, K=4):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{problem}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\"}]\n",
    "    input_message = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    gens = [\n",
    "        model.generate(input_tensor, max_new_tokens=2048, num_beams=4, top_p=0.9)\n",
    "        for _ in range(K)\n",
    "    ]\n",
    "    outputs = [tokenizer.decode(g[0][input_tensor.shape[1]:], skip_special_tokens=True) for g in gens]\n",
    "\n",
    "    rewards = torch.tensor(\n",
    "        [reward_func(input_message, out, true_answer) for out in outputs],\n",
    "        dtype=torch.float32, device=model.device\n",
    "    )\n",
    "\n",
    "    rel_rewards = rewards - rewards.mean()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_out = model.generate(input_tensor, max_new_tokens=256, temperature=0.7, do_sample=True, top_k=10)\n",
    "        ref_text = tokenizer.decode(ref_out[0], skip_special_tokens=True)\n",
    "        ref_r = reward_func(input_message, ref_text, true_answer)\n",
    "    final_rewards = rel_rewards - ref_r\n",
    "\n",
    "    loss = 0.0\n",
    "    for out, r in zip(outputs, final_rewards):\n",
    "        tokens = tokenizer(input_message + out, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        out = model(**tokens, labels=tokens[\"input_ids\"])\n",
    "        logprobs = out.loss  # NLL\n",
    "        loss += r * logprobs\n",
    "\n",
    "    loss = loss / K\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return outputs, rewards.tolist(), ref_text, ref_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3XJERTMraK2",
    "outputId": "1364fd54-9fb5-417a-bd25-136421156bfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Rewards: [1.0, 1.0, 1.0, 1.0] | Ref reward: 1\n",
      "Find all values of $x$ such that $\\displaystyle\\frac{1}{x-1} + \\frac{2x}{x - 1} = 5$. \n",
      "\n",
      "Sample Solution: We can combine the two terms on the left side to get $\\dfrac{1+2x}{x-1} = 5$.  We then multiply both sides of this equation by $x-1$ to get rid of the fractions.   This gives us $1+2x = 5(x-1)$. Expanding the right side gives $1+2x = 5x -5$.  Subtracting $5x$ from both sides   gives $1-3x = -5$, and subtracting 1 from both sides of this equation yields $-3x = -6$.  Dividing both sides   of this equation by $-3$ gives us our answer, $x = \\boxed{2}$. \n",
      "\n",
      "Sample output: Step 1: The given equation is $\\displaystyle\\frac{1}{x-1} + \\frac{2x}{x - 1} = 5$. The first step is to simplify the equation by combining the fractions on the left-hand side.\n",
      "\n",
      "Step 2: Since both fractions have the same denominator $(x-1)$, we can combine them into a single fraction: $\\displaystyle\\frac{1 + 2x}{x-1} = 5$.\n",
      "\n",
      "Step 3: Next, we can multiply both sides of the equation by $(x-1)$ to eliminate the denominator. This gives us $1 + 2x = 5(x-1)$.\n",
      "\n",
      "Step 4: Now, we can expand the right-hand side of the equation: $1 + 2x = 5x - 5$.\n",
      "\n",
      "Step 5: Next, we can move all terms involving $x$ to one side of the equation by subtracting $2x$ from both sides: $1 = 3x - 5$.\n",
      "\n",
      "Step 6: Then, we can add $5$ to both sides of the equation to isolate the term involving $x$: $6 = 3x$.\n",
      "\n",
      "Step 7: Finally, we can divide both sides of the equation by $3$ to solve for $x$: $x = \\frac{6}{3} = 2$.\n",
      "\n",
      "Therefore, the value of $x$ that satisfies the equation is $\\boxed{2}$. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Rewards: [0.0, 0.0, 0.0, 0.0] | Ref reward: 0\n",
      "Find all solutions to the equation \\[\\frac{\\left(\\frac{x}{x+1}\\right)^2 + 11}{\\left(\\frac{x}{x+1}\\right)^2 + 1} = 2.\\]Enter all the solutions, separated by commas. \n",
      "\n",
      "Sample Solution: We make the substitution $y = \\left(\\frac{x}{x+1}\\right)^2$ to simplify the equation, so that \\[\\frac{y+11}{y+1} = 2.\\]Multiplying by $y+1$ gives $y+11 = 2y+2,$ so $y=9.$ Therefore, we have \\[\\frac{x}{x+1} = \\pm 3.\\]Then, either $x = 3(x+1)$ or $x = -3(x+1).$ These give solutions $x =\\boxed{ -\\tfrac32}$ and $x = \\boxed{-\\tfrac34},$ respectively. \n",
      "\n",
      "Sample output: Step 1: Multiply both sides of the equation by the denominator to eliminate the fraction.\n",
      "\n",
      "\\[\\frac{\\left(\\frac{x}{x+1}\\right)^2 + 11}{\\left(\\frac{x}{x+1}\\right)^2 + 1} = 2\\]\n",
      "\n",
      "\\[\\left(\\frac{x}{x+1}\\right)^2 + 11 = 2\\left(\\left(\\frac{x}{x+1}\\right)^2 + 1\\right)\\]\n",
      "\n",
      "Step 2: Expand the right-hand side of the equation.\n",
      "\n",
      "\\[\\left(\\frac{x}{x+1}\\right)^2 + 11 = 2\\left(\\frac{x^2}{(x+1)^2}\\right) + 2\\]\n",
      "\n",
      "\\[\\left(\\frac{x}{x+1}\\right)^2 + 11 = \\frac{2x^2}{(x+1)^2} + 2\\]\n",
      "\n",
      "Step 3: Multiply both sides of the equation by $(x+1)^2$ to eliminate the fraction.\n",
      "\n",
      "\\[(x)^2 + 11(x+1)^2 = 2x^2 + 2(x+1)^2\\]\n",
      "\n",
      "Step 4: Expand both sides of the equation.\n",
      "\n",
      "\\[x^2 + 11x^2 + 22x + 11 = 2x^2 + 2x^2 + 4x + 2\\]\n",
      "\n",
      "Step 5: Combine like terms.\n",
      "\n",
      "\\[12x^2 + 22x + 11 = 4x^2 + 4x + 2\\]\n",
      "\n",
      "Step 6: Move all terms to one side of the equation.\n",
      "\n",
      "\\[12x^2 + 22x + 11 - 4x^2 - 4x - 2 = 0\\]\n",
      "\n",
      "\\[8x^2 + 18x + 9 = 0\\]\n",
      "\n",
      "Step 7: Factor the quadratic equation.\n",
      "\n",
      "\\[(4x + 3)(2x + 3) = 0\\]\n",
      "\n",
      "Step 8: Solve for x.\n",
      "\n",
      "\\[4x + 3 = 0 \\quad \\text{or} \\quad 2x + 3 = 0\\]\n",
      "\n",
      "\\[4x = -3 \\quad \\text{or} \\quad 2x = -3\\]\n",
      "\n",
      "\\[x = -\\frac{3}{4} \\quad \\text{or} \\quad x = -\\frac{3}{2}\\]\n",
      "\n",
      "Therefore, the solutions to the equation are $x = -\\frac{3}{4}$ and $x = -\\frac{3}{2}$.\n",
      "\n",
      "\\boxed{-\\frac{3}{4}, -\\frac{3}{2}} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Rewards: [1.0, 1.0, 1.0, 1.0] | Ref reward: 1\n",
      "What is $88 \\div (4 \\div 2)$? \n",
      "\n",
      "Sample Solution: We perform the operation in parentheses first: \\[88 \\div (4 \\div 2) = 88 \\div 2 = \\boxed{44}.\\] \n",
      "\n",
      "Sample output: To evaluate the expression $88 \\div (4 \\div 2)$, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expression inside the parentheses: $4 \\div 2 = 2$\n",
      "2. Now the expression becomes $88 \\div 2$\n",
      "3. Divide 88 by 2: $88 \\div 2 = 44$\n",
      "\n",
      "Therefore, the final answer is: $\\boxed{44}$ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Rewards: [0.0, 0.0, 0.0, 0.0] | Ref reward: 0\n",
      "The equations\n",
      "\\[75x^4 + ax^3 + bx^2 + cx + 12 = 0\\]and\n",
      "\\[12x^5 + dx^4 + ex^3 + fx^2 + gx + 75 = 0\\]have a common rational root $k$ which is not an integer, and which is negative.  What is $k?$ \n",
      "\n",
      "Sample Solution: Let $k = \\frac{m}{n}$ in reduced form, where $m$ and $n$ are integers.  Then by the Rational Root Theorem, $m$ divides 12 and $m$ divides 75, so $m$ must divide $\\gcd(12,75) = 3.$  Similarly, $n$ divides 75 and $n$ divides 12, so $n$ must divide $\\gcd(75,12) = 3.$  Thus, $m,$ $n \\in \\{-3, -1, 1, 3\\}.$\n",
      "\n",
      "We are told that $k = \\frac{m}{n}$ is not an integer, and negative.  The only possibility is that $k =\\boxed{-\\frac{1}{3}}.$ \n",
      "\n",
      "Sample output: The Rational Root Theorem states that if a rational number $p/q$ is a root of the polynomial equation $a_nx^n + a_{n-1}x^{n-1} + \\dots + a_1x + a_0 = 0$, then $p$ must be a factor of $a_0$ and $q$ must be a factor of $a_n$.\n",
      "\n",
      "For the first equation $75x^4 + ax^3 + bx^2 + cx + 12 = 0$, the factors of $a_0 = 12$ are $\\pm 1, \\pm 2, \\pm 3, \\pm 4, \\pm 6, \\pm 12$, and the factors of $a_n = 75$ are $\\pm 1, \\pm 3, \\pm 5, \\pm 15, \\pm 25, \\pm 75$.\n",
      "\n",
      "For the second equation $12x^5 + dx^4 + ex^3 + fx^2 + gx + 75 = 0$, the factors of $a_0 = 75$ are $\\pm 1, \\pm 3, \\pm 5, \\pm 15, \\pm 25, \\pm 75$, and the factors of $a_n = 12$ are $\\pm 1, \\pm 2, \\pm 3, \\pm 4, \\pm 6, \\pm 12$.\n",
      "\n",
      "Since $k$ is a common rational root of both equations and is not an integer, it must be of the form $p/q$ where $p$ is a factor of both $12$ and $75$, and $q$ is a factor of both $75$ and $12$.\n",
      "\n",
      "The only common factors of $12$ and $75$ are $\\pm 1$ and $\\pm 3$, so the possible values of $p$ are $\\pm 1$ and $\\pm 3$.\n",
      "\n",
      "The only common factors of $75$ and $12$ are $\\pm 1$ and $\\pm 3$, so the possible values of $q$ are $\\pm 1$ and $\\pm 3$.\n",
      "\n",
      "Therefore, the possible values of $k$ are $\\pm 1/1$, $\\pm 1/3$, $\\pm 3/1$, and $\\pm 3/3$, which simplifies to $\\pm 1$, $\\pm 1/3$, $\\pm 3$, and $\\pm 1$.\n",
      "\n",
      "Since $k$ is not an integer and is negative, the only possible value of $k$ is $-1/3$. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Rewards: [0.0, 0.0, 0.0, 0.0] | Ref reward: 0\n",
      "Find all positive real numbers $x$ that satisfy\n",
      "\\[x \\sqrt{12 - x} + \\sqrt{12x - x^3} \\ge 12.\\]Enter all solutions, separated by commas. \n",
      "\n",
      "Sample Solution: We write\n",
      "\\[x \\sqrt{12 - x} + \\sqrt{12x - x^3} = \\sqrt{12 - x} \\cdot \\sqrt{x^2} + \\sqrt{x} \\cdot \\sqrt{12 - x^2}\\]By Cauchy-Schwarz,\n",
      "\\[(\\sqrt{12 - x} \\cdot \\sqrt{x^2} + \\sqrt{x} \\cdot \\sqrt{12 - x^2})^2 \\le (12 - x + x)(x^2 + 12 - x^2) = 144,\\]so\n",
      "\\[\\sqrt{12 - x} \\cdot \\sqrt{x^2} + \\sqrt{x} \\cdot \\sqrt{12 - x^2} \\le 12.\\]But $\\sqrt{12 - x} \\cdot \\sqrt{x^2} + \\sqrt{x} \\cdot \\sqrt{12 - x^2} \\ge 12,$ so the expression must be equal to 12.  From the equality condition for Cauchy-Schwarz,\n",
      "\\[\\frac{12 - x}{x} = \\frac{x^2}{12 - x^2}.\\]Then $(12 - x)(12 - x^2) = x^3,$ which simplifies to $x^2 + x - 12 = 0.$  This factors as $(x - 3)(x + 4) = 0,$ so the only solution is $x = \\boxed{3}.$ \n",
      "\n",
      "Sample output: Step 1:  We are given the inequality $x \\sqrt{12 - x} + \\sqrt{12x - x^3} \\ge 12.$ Our goal is to find all positive real numbers $x$ that satisfy this inequality.\n",
      "Step 2:  To approach this problem, we can start by squaring both sides of the given inequality. This will help simplify the expression and make it easier to work with.\n",
      "Step 3:  After squaring both sides, we get $x^2 (12 - x) + 2x \\sqrt{(12 - x)(12x - x^3)} + 12x - x^3 \\ge 144.$\n",
      "Step 4:  Simplifying further, we obtain $x^2 (12 - x) + 2x \\sqrt{(12 - x)(12x - x^3)} + 12x - x^3 - 144 \\ge 0.$\n",
      "Step 5:  Rearranging the terms, we have $x^2 (12 - x) + 12x - x^3 + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 6:  Factoring out common terms, we get $x^2 (12 - x) - x^3 + 12x + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 7:  Further simplifying, we arrive at $x^2 (12 - x - x) + 12x + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 8:  This simplifies to $x^2 (12 - 2x) + 12x + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 9:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 10:  Rearranging the terms, we have $12x (x - 6) - 144 + 2x \\sqrt{(12 - x)(12x - x^3)} \\ge 0.$\n",
      "Step 11:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 12:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 13:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 14:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 15:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 16:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 17:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 18:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 19:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 20:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 21:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 22:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 23:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 24:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 25:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 26:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 27:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 28:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 29:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 30:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 31:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 32:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 33:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 34:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 35:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 36:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 37:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 38:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 39:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 40:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 41:  Factoring out common terms, we get $12x (x - 6) + 2x \\sqrt{(12 - x)(12x - x^3)} - 144 \\ge 0.$\n",
      "Step 42:  Further simplifying, we arrive at $12x (x - 6) + 2x \\sqrt{(12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for step in range(5):  # demo loop\n",
    "    sample = random.choice(dataset)\n",
    "    outs, rs, ref, ref_r = grpo_rlvr_step(sample[\"problem\"], sample[\"solution\"], K=4)\n",
    "    print(f\"Step {step} | Rewards: {rs} | Ref reward: {ref_r}\")\n",
    "    print(sample[\"problem\"], \"\\n\")\n",
    "    print(\"Sample Solution:\", sample[\"solution\"], \"\\n\")\n",
    "    print(\"Sample output:\", outs[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (My rl_rose Kernel)",
   "language": "python",
   "name": "rl_rose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "044a12774231424eb85419b2085f27b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fc5c811c1e04ebf89a9343496b251ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "187a2660573e43a599c37c1bd2f15e11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c1a8717b68146f9a22610fdc3ec8ba3",
      "placeholder": "​",
      "style": "IPY_MODEL_044a12774231424eb85419b2085f27b4",
      "value": " 12500/12500 [00:00&lt;00:00, 20171.21 examples/s]"
     }
    },
    "1a5fc6ac86b248ecb600b246013ce900": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c1a8717b68146f9a22610fdc3ec8ba3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "584260c925be4ba4afa16dc147344bd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fb577ecf6714a1baff199f0cca72faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6330b991bf084f93b3dc2cef1539ace6",
      "max": 12500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_584260c925be4ba4afa16dc147344bd0",
      "value": 12500
     }
    },
    "6330b991bf084f93b3dc2cef1539ace6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d4a25f5d88748f8ba515e9d6258a700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cca0ce8c9d814e0ab32b4d73c2acb976",
      "placeholder": "​",
      "style": "IPY_MODEL_1a5fc6ac86b248ecb600b246013ce900",
      "value": "Map: 100%"
     }
    },
    "cca0ce8c9d814e0ab32b4d73c2acb976": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee841deb24694baaa455011fccc0a9c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d4a25f5d88748f8ba515e9d6258a700",
       "IPY_MODEL_5fb577ecf6714a1baff199f0cca72faa",
       "IPY_MODEL_187a2660573e43a599c37c1bd2f15e11"
      ],
      "layout": "IPY_MODEL_0fc5c811c1e04ebf89a9343496b251ed"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
